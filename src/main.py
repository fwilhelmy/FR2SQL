"""Simple interactive pipeline linking natural language questions to a SQL
schema and producing a prompt for the future LLM agent."""

from typing import List, Dict

from data.SchemaExtractor import SchemaExtractor
from data.SchemaQuestionLinker import schema_link, TRESHOLD
from pipeline.ContextGenerator import ContextGenerator


DB_PATH = "data/sqlite/employee_db.sqlite"
# Threshold used to decide whether we are confident enough in the
# schema linking step.  This is separate from the TRESHOLD constant of
# the linker which filters individual matches.
CONFIDENCE_THRESHOLD = 75


def ask_question() -> str:
    """Prompt the user for a question in natural language."""
    return input("Question: ").strip()


def ask_clarification() -> str:
    """Request a clarification from the user when confidence is low."""
    return input("Clarification: ").strip()


def generate_sql(_prompt: str) -> str:
    """Placeholder for the NL2SQL model call."""
    # TODO: integrate the actual LLM call here
    print("\n[LLM PROMPT]\n" + _prompt + "\n")
    return "-- TODO: SQL query generated by LLM"


def execute_sql(_sql: str, _db_path: str) -> None:
    """Placeholder for query execution against the database."""
    # TODO: implement real SQL execution and result handling
    print(f"Would execute on {_db_path}: {_sql}")


def compute_average(scores: List[float]) -> float:
    return sum(scores) / len(scores) if scores else 0.0


def main() -> None:
    """Run the end‑to‑end demo pipeline."""

    question = ask_question()

    extractor = SchemaExtractor(DB_PATH)
    schema_pairs: Dict[str, List[str]] = extractor.extract_column_table_pairs()

    # Flatten table and column names for the linker
    schema_elements: List[str] = list(schema_pairs.keys()) + [
        col for cols in schema_pairs.values() for col in cols
    ]

    # Example corpus of previous user requests.  In a real system this would be
    # replaced with stored logs or documentation.
    corpus = [
        "montre-moi le salaire moyen par département",
        "liste des employés embauchés après 2015",
        "nombre total de projets par manager",
    ]

    matches = schema_link(question, corpus, schema_elements)
    matches.sort(key=lambda m: m["score"], reverse=True)

    selected = []
    selected_names = []
    for m in matches:
        if m["schema_element"] not in selected_names and m["score"] > TRESHOLD:
            selected.append(m)
            selected_names.append(m["schema_element"])

    avg_score = compute_average([m["score"] for m in selected])

    if avg_score < CONFIDENCE_THRESHOLD:
        # Confidence too low → ask for clarification
        question += " " + ask_clarification()

    # Determine which tables were referenced
    tables = [name for name in selected_names if name in schema_pairs]
    table_metadata = {
        t: extractor.extract_table_metadata(t) for t in tables
    }

    # Convert to the format expected by ContextGenerator
    schema_for_prompt = {"tables": []}
    for t, meta in table_metadata.items():
        schema_for_prompt["tables"].append({
            "name": t,
            "columns": meta["columns"],
        })

    context_gen = ContextGenerator(None, None)
    prompt = context_gen.generate_llm_context(schema_for_prompt, question)

    sql = generate_sql(prompt)
    execute_sql(sql, DB_PATH)


if __name__ == "__main__":
    main()

