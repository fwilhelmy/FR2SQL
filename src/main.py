"""Simple interactive pipeline linking natural language questions to a SQL
schema and producing a prompt for the future LLM agent."""

from typing import List, Dict

from database.DBManager import DBManager
from database.SchemaQuestionLinker import schema_link, TRESHOLD
from agent.PromptGenerator import generate_llm_context


DB_PATH = "data/sqlite/employee_db.sqlite"
# Threshold used to decide whether we are confident enough in the
# schema linking step.  This is separate from the TRESHOLD constant of
# the linker which filters individual matches.
CONFIDENCE_THRESHOLD = 75


def ask_question() -> str:
    """Prompt the user for a question in natural language."""
    return input("Question: ").strip()


def ask_clarification() -> str:
    """Request a clarification from the user when confidence is low."""
    return input("Clarification: ").strip()


def generate_sql(_prompt: str) -> str:
    """Placeholder for the NL2SQL model call."""
    # TODO: integrate the actual LLM call here
    print("\n[LLM PROMPT]\n" + _prompt + "\n")
    return "-- TODO: SQL query generated by LLM"


def execute_sql(sql: str, db: DBManager) -> None:
    """Execute the generated SQL and print the results."""
    result = db.execute_query(sql)
    if "columns" in result:
        header = " | ".join(result["columns"])
        print(header)
        print("-" * len(header))
        for row in result["rows"]:
            print(" | ".join(str(item) for item in row))
    else:
        print(f"{result['rowcount']} row(s) affected")


def compute_average(scores: List[float]) -> float:
    return sum(scores) / len(scores) if scores else 0.0


def main() -> None:
    """Run the end‑to‑end demo pipeline."""

    question = ask_question()

    db = DBManager(DB_PATH)
    schema_pairs: Dict[str, List[str]] = db.extract_column_table_pairs()

    # Flatten table and column names for the linker
    schema_elements: List[str] = list(schema_pairs.keys()) + [
        f"{col} {table}" for table, cols in schema_pairs.items() for col in cols
    ]

    # Example corpus of previous user requests.  In a real system this would be
    # replaced with stored logs or documentation.
    corpus = [
        "montre-moi le salaire moyen par département",
        "liste des employés embauchés après 2015",
        "nombre total de projets par manager",
    ]
    # Afficher le total des ventes par ville pour l'année 2023.
    matches = schema_link(question, corpus, schema_elements)
    matches.sort(key=lambda m: m["score"], reverse=True)

    selected = []
    selected_tables = []
    for m in matches:
        meta = m["schema_element"].split(" ")
        if len(meta) > 1:
            m["schema_table"] = meta[1]
            m["schema_column"] = meta[0]
        else:
            m["schema_table"] = meta[0]
        if m["schema_table"] not in selected_tables and m["score"] > TRESHOLD:
            selected.append(m)
            selected_tables.append(m["schema_table"])
        print(f"Matched: '{m['keyword']}' → '{m['schema_table']}.{m.get('schema_column',"")}' ({m['score']}%)")

    avg_score = compute_average([m["score"] for m in selected])

    if avg_score < CONFIDENCE_THRESHOLD:
        # Confidence too low → ask for clarification
        question += " " + ask_clarification()

    # Determine which tables were referenced
    table_metadata = {t: db.extract_table_metadata(t) for t in selected_tables}

    # Convert to the format expected by ContextGenerator
    schema_for_prompt = {"tables": []}
    for t, meta in table_metadata.items():
        schema_for_prompt["tables"].append({
            "name": t,
            "columns": meta["columns"],
        })

    prompt = generate_llm_context(schema_for_prompt, question, db_type="sqlite")

    sql = generate_sql(prompt)
    execute_sql(sql, db)
    db.close()


if __name__ == "__main__":
    main()

